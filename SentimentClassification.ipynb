{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c2df8a9",
   "metadata": {},
   "source": [
    "# Sentiment Identification \n",
    "\n",
    "BACKGROUND: A large multinational corporation is seeking to automatically identify the sentiment that their customer base\n",
    "    talks about on social media. They would like to expand this capability into multiple languages. Many 3rd party tools\n",
    "    exist for sentiment analysis; however, they need help with under-resourced languages. \n",
    "\n",
    "GOAL: Train a sentiment classifier (Positive, Negative, Neutral) on a corpus of the provided documents. Your goal is to\n",
    "    maximize accuracy. There is special interest in being able to accurately detect negative sentiment. The training data \n",
    "    includes documents from a wide variety of sources, not merely social media, and some of it may be inconsistently labeled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96a26219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from cleantext import clean\n",
    "import emoji\n",
    "from matplotlib.pyplot import plot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0073437",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380ac6d",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad00b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./roman+urdu+data+set/Roman Urdu DataSet.csv\",header=None, usecols=[0,1], names=[\"text\",\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3171e727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sai kha ya her kisi kay bus ki bat nhi hai lakin main ki hal kal bi Aj aur aj bi sirf Aus say bus</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sahi bt h</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kya bt hai,</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wah je wah</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are wha kaya bat hai</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wah kya baat likhi</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wha Itni sari khubiya</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Itni khubiya</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ya allah rehm farma hm sab pe or zalimo ko hidayat de ameen</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Please Everyone AllAh S.w.T ka naam hAmesha Bary Lawzo main Likha kary Wo he Zaat sUb say Bari Hey</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                 text  \\\n",
       "0   Sai kha ya her kisi kay bus ki bat nhi hai lakin main ki hal kal bi Aj aur aj bi sirf Aus say bus   \n",
       "1                                                                                           sahi bt h   \n",
       "2                                                                                         Kya bt hai,   \n",
       "3                                                                                          Wah je wah   \n",
       "4                                                                                Are wha kaya bat hai   \n",
       "5                                                                                  Wah kya baat likhi   \n",
       "6                                                                               Wha Itni sari khubiya   \n",
       "7                                                                                        Itni khubiya   \n",
       "8                                         Ya allah rehm farma hm sab pe or zalimo ko hidayat de ameen   \n",
       "9  Please Everyone AllAh S.w.T ka naam hAmesha Bary Lawzo main Likha kary Wo he Zaat sUb say Bari Hey   \n",
       "\n",
       "  Sentiment  \n",
       "0  Positive  \n",
       "1  Positive  \n",
       "2  Positive  \n",
       "3  Positive  \n",
       "4  Positive  \n",
       "5  Positive  \n",
       "6  Positive  \n",
       "7  Positive  \n",
       "8  Positive  \n",
       "9  Positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaad5e3b",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfa05d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20229, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a057b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20229 entries, 0 to 20228\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       20228 non-null  object\n",
      " 1   Sentiment  20229 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 316.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff314697",
   "metadata": {},
   "source": [
    "It is observed that there is one null value for text in this dataframe. We will remove this row after cleaning the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9768dc60",
   "metadata": {},
   "source": [
    "After reviewing the data closely, I observed the following. \n",
    "1.\tEnglish sentences\n",
    "2.\tEnglish words intermixed with urdu\n",
    "3.\tCharacters √∞≈∏Àú, Emojis\n",
    "4.\tDifferent spellings for same word- aunty vs anty vs Aunti\n",
    "5.\tColloquial language in English mixed in\n",
    "6.\tShort forms- Mbrk for Mubarak\n",
    "7.\tUnnecessary punctuation- ,,,\n",
    "8.\t  üòä is noted as negative- Row 10595\n",
    "9.\t ‚Äú Hahaha‚Ä¶ so true ‚Äú is noted as negative- Row 10597 \n",
    "10.\t Row 10615 : Well done hahaha is noted as negative. This is incorrect.\n",
    "11.\tMany words are not spelled correctly. ‚Äúhar‚Äù is hr (meaning \"all\"), \"bat\" (pronounced as bhath, meaning \"talk\") appears as both \"bat\"(Row 18604) and \"bt\" (Row 2,3)\n",
    "\n",
    "We can remove special characters, punctuation, replace emojis with words, convert all words to lower case. Characters repeated 3 times or more are replaced by just two instances. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e1eb73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emojis_with_words(text):\n",
    "    # Emoji replacement dictionary\n",
    "    emoji_map = {\n",
    "        r\":-?\\)\": \"happy\",\n",
    "        r\":-?[dD]\": \"happy\",\n",
    "        r\":-?\\(\": \"sad\",\n",
    "        r\":-?[Pp]\": \"playful\",\n",
    "        r\";-?\\)\": \"wink\",\n",
    "        r\":'\\(\": \"crying\",\n",
    "        r\":-?O\": \"surprised\",\n",
    "        r\":-?o\": \"surprised\",\n",
    "        r\":-?v\": \"awkward\",\n",
    "        r\"<3\": \"love\",\n",
    "        r\"xD\": \"laughing\",\n",
    "        r\"X-D\": \"laughing\",\n",
    "        r\":-?\\|\": \"neutral\",\n",
    "        r\"[oO]_[oO]\": \"shocked\",\n",
    "        r\";_;\": \"crying\"\n",
    "    }\n",
    "    for pattern, word in emoji_map.items():\n",
    "        text = re.sub(pattern, word, text, flags=re.IGNORECASE)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f69117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(text, \n",
    "                 lower: bool = False, \n",
    "                 punct: bool = False):\n",
    "    return clean(text=text,\n",
    "            fix_unicode=True,\n",
    "            to_ascii=True,\n",
    "            lower=lower,\n",
    "            no_line_breaks=False,\n",
    "            no_urls=False,\n",
    "            no_emails=False,\n",
    "            no_phone_numbers=False,\n",
    "            no_numbers=False,\n",
    "            no_digits=False,\n",
    "            no_currency_symbols=False,\n",
    "            no_punct=punct,\n",
    "            lang=\"en\"\n",
    "            )\n",
    "\n",
    "def remove_repeated_chars(text):\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "    return text\n",
    "def reduce_repeated_syllables(text):\n",
    "    # Reduce any 3 or more repetitions of a character group to just 2\n",
    "    return re.sub(r'(\\b\\w{1,}?)(\\1{2,})', r'\\1\\1', text)\n",
    "\n",
    "\n",
    "df[\"cleaned\"] = df[\"text\"].apply(remove_chars)\n",
    "df[\"cleaned\"] = df[\"cleaned\"].apply(replace_emojis_with_words)\n",
    "df[\"cleaned\"] = df[\"cleaned\"].apply(remove_repeated_chars)\n",
    "df[\"cleaned\"] = df[\"cleaned\"].apply(reduce_repeated_syllables)\n",
    "df[\"cleaned\"] = df[\"cleaned\"].apply(remove_chars,lower = True, punct = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29259bf9",
   "metadata": {},
   "source": [
    "A quick check to see if cleaning was successful for emojis, punctuations, converting to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f47b4b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text               :)\n",
       "Sentiment    Negative\n",
       "cleaned         happy\n",
       "Name: 10594, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[10594,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a14c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                 Waqar Khan :p yahan bhi noon league :D\n",
       "Sentiment                                           Neutral\n",
       "cleaned      waqar khan playful yahan bhi noon league happy\n",
       "Name: 18694, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[18694,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d845a863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         Kya bt hai,\n",
       "Sentiment       Positive\n",
       "cleaned       kya bt hai\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de21e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\betti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3edc7da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = ['ai', 'ayi', 'hy', 'hai', 'main', 'ki', 'tha', 'koi', 'ko', 'sy', 'woh', 'bhi', 'aur', 'wo', 'yeh', \n",
    "                  'rha', 'hota', 'ho', 'ga', 'ka', 'le', 'lye', 'kr', 'kar', 'lye', 'liye', 'hotay', 'waisay', 'gya', \n",
    "                  'gaya', 'kch', 'ab', 'thy', 'thay', 'houn', 'hain', 'han', 'to', 'is', 'hi', 'jo', 'kya', 'thi', 'se',\n",
    "                  'pe', 'phr', 'wala', 'waisay', 'us', 'na', 'ny', 'hun', 'rha', 'raha', 'ja', 'rahay', 'abi', 'uski',\n",
    "                  'ne', 'haan', 'acha', 'nai', 'sent', 'photo', 'you', 'kafi', 'gai', 'rhy', 'kuch', 'jata', 'aye', 'ya',\n",
    "                  'dono', 'hoa', 'aese', 'de', 'wohi', 'jati', 'jb', 'krta', 'lg', 'rahi', 'hui', 'karna', 'krna', 'gi',\n",
    "                  'hova', 'yehi', 'jana', 'jye', 'chal', 'mil', 'tu', 'hum', 'par', 'hay', 'kis', 'sb', 'gy', 'dain', \n",
    "                  'krny', 'tou']\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "df[\"cleaned\"] = df[\"cleaned\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "259b9c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Drop any rows with missing values\n",
    "df.dropna(subset=[\"cleaned\", \"text\",\"Sentiment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d58181e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20228, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf5ac3",
   "metadata": {},
   "source": [
    "Let us understand the dataset some more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a75709ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neative</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>5286</td>\n",
       "      <td>5286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>8928</td>\n",
       "      <td>8928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>6013</td>\n",
       "      <td>6013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text  cleaned\n",
       "Sentiment               \n",
       "Neative       1        1\n",
       "Negative   5286     5286\n",
       "Neutral    8928     8928\n",
       "Positive   6013     6013"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Sentiment\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b163d2e",
   "metadata": {},
   "source": [
    "The Sentiment Neative seems to be a typo for Negative. Replacing Neative with Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad554f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.Sentiment==\"Neative\",\"Sentiment\"] = \"Negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600c3467",
   "metadata": {},
   "source": [
    "Plot breakdown of sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a6911f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Sentiment Count'}, xlabel='Sentiment', ylabel='Count'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped = df.groupby(\"Sentiment\")[\"text\"].count()\n",
    "grouped.plot(kind='bar', xlabel = \"Sentiment\", ylabel = \"Count\", title = \"Sentiment Count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd34212",
   "metadata": {},
   "source": [
    "There seems to be a large number of Neutral Sentiment compared to both Positive and Negative. The company would likely\n",
    "care more about Negative ratings than Neutral or Positive ratings if their goal is to improve customer sentiment towards\n",
    "them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b95c6",
   "metadata": {},
   "source": [
    "### Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009454c9",
   "metadata": {},
   "source": [
    "The Roman Urdu dataset is all text. This needs to be encoded into numerical values. There are only 3 sentiment values and \n",
    "they can be encoded using Label Encoder. The text field can be encoded in a number of ways- Count Vectorizer, TF-IDF (Term Frequency Inverse Document Frequency) etc. Neural nets and advanced models such as bert and mbert can be used to encode the entire dataset. \n",
    "\n",
    "I will use CountVectorizer and TF-IDF to encode the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17d0f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "le =LabelEncoder()\n",
    "df[\"Sentiment_encoded\"]=le.fit_transform(df[\"Sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5f70b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Sentiment_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f6c59a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3762</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10100</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Sentiment  Sentiment_encoded\n",
       "0      0  Positive                  2\n",
       "1   3762   Neutral                  1\n",
       "2  10100  Negative                  0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Sentiment\", \"Sentiment_encoded\"]].drop_duplicates(\"Sentiment\").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6ed41",
   "metadata": {},
   "source": [
    "Negative sentiment is encoded as 0, Neutral as 1, Positive as 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9da5b7",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a326d12",
   "metadata": {},
   "source": [
    "It is important to split the data into test and training set to measure the performance of the models. The problem states that accuracy is the benchmark. Accuracy will need to be measured on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4638be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3d3853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc7c6c",
   "metadata": {},
   "source": [
    "We can start with Logistic Regression and Naive Bayes, both commonly used for text classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603bb5e",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6023d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=10, max_iter= 1000, solver=\"saga\", class_weight=\"balanced\", multi_class=\"multinomial\")  \n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f4cc752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score 0.6260504201680672\n"
     ]
    }
   ],
   "source": [
    "#tf-idf + LR\n",
    "print(\"Accuracy_score\" ,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06c7ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\betti\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1476: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 1, 'clf__penalty': 'l2'}\n",
      "Best Cross-Validation Accuracy: 0.6441732509576197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.54      0.55      1080\n",
      "           1       0.64      0.69      0.67      1720\n",
      "           2       0.66      0.61      0.63      1246\n",
      "\n",
      "    accuracy                           0.63      4046\n",
      "   macro avg       0.62      0.61      0.62      4046\n",
      "weighted avg       0.63      0.63      0.63      4046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('clf', LogisticRegression(random_state=10, solver=\"saga\", class_weight=\"balanced\", l1_ratio = 0.5, multi_class=\"multinomial\"))  # logistic regression model\n",
    "])\n",
    "param_grid = {\n",
    "    'clf__C': [0.1, 1, 5, 10],  # Regularization strength\n",
    "    'clf__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "}\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1b1ca",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94c09833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cccc89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6238ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.toarray()\n",
    "x_test = X_test.toarray()\n",
    "#Train the model using the training sets\n",
    "mnb.fit(x_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = mnb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53bcff16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score 0.5949085516559565\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy_score\" ,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773982ad",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f8f933e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train SVM classifier\n",
    "svm_classifier = SVC(class_weight = \"balanced\")\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d05a76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score 0.6248146317350469\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(\"Accuracy_score\" ,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520f726",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2c0ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb411fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\betti\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1476: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'clf__C': 5, 'clf__penalty': 'l2'}\n",
      "Best Cross-Validation Accuracy: 0.6504768145511589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.54      0.56      1080\n",
      "           1       0.65      0.73      0.68      1720\n",
      "           2       0.66      0.60      0.63      1246\n",
      "\n",
      "    accuracy                           0.64      4046\n",
      "   macro avg       0.63      0.62      0.63      4046\n",
      "weighted avg       0.64      0.64      0.64      4046\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\betti\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)\n",
    "pipeline = Pipeline([\n",
    "    ('clf', LogisticRegression(random_state=10, solver=\"saga\", class_weight=\"balanced\", l1_ratio = 0.5, multi_class=\"multinomial\"))  # logistic regression model\n",
    "])\n",
    "param_grid = {\n",
    "    'clf__C': [0.1, 1, 5, 10],  # Regularization strength\n",
    "    'clf__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "}\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = grid.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91f18f",
   "metadata": {},
   "source": [
    "Having tried multiple machine learning models including the three showcased in this notebook, I find that Logistic Regression gave the best accuracy without hyperparameter tuning. The hyperparameter tuning gives slightly better results. Using CountVectorizer instead of TF-IDF gives slightly better accuracy as well. For SVM and Naive Bayes, one can do hyperparameter tuning to get some improvement in accuracies. I am choosing TF-IDF + Logistic Regression moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9501a8e5",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812cfeac",
   "metadata": {},
   "source": [
    "Given that the company would be most interested in the negative reviews, one could\n",
    "1. use just Positive and Negative reviews since the numbers for both are comparable to each other.\n",
    "2. Combine the Positive and neutral ratings into one category and do a binary classification vs the Negative ratings.\n",
    "\n",
    "I will test out a simple Logistic Regression for both scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f6e54",
   "metadata": {},
   "source": [
    "### Use only Positive and Negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2fb949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pn = df.loc[((df.Sentiment == \"Positive\") | (df.Sentiment == \"Negative\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a597c839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11300, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3305b08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>Sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sai kha ya her kisi kay bus ki bat nhi hai lakin main ki hal kal bi Aj aur aj bi sirf Aus say bus</td>\n",
       "      <td>Positive</td>\n",
       "      <td>sai kha her kisi kay bus bat nhi lakin hal kal bi aj aj bi sirf aus say bus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sahi bt h</td>\n",
       "      <td>Positive</td>\n",
       "      <td>sahi bt h</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kya bt hai,</td>\n",
       "      <td>Positive</td>\n",
       "      <td>bt</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wah je wah</td>\n",
       "      <td>Positive</td>\n",
       "      <td>wah je wah</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are wha kaya bat hai</td>\n",
       "      <td>Positive</td>\n",
       "      <td>are wha kaya bat</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                text  \\\n",
       "0  Sai kha ya her kisi kay bus ki bat nhi hai lakin main ki hal kal bi Aj aur aj bi sirf Aus say bus   \n",
       "1                                                                                          sahi bt h   \n",
       "2                                                                                        Kya bt hai,   \n",
       "3                                                                                         Wah je wah   \n",
       "4                                                                               Are wha kaya bat hai   \n",
       "\n",
       "  Sentiment  \\\n",
       "0  Positive   \n",
       "1  Positive   \n",
       "2  Positive   \n",
       "3  Positive   \n",
       "4  Positive   \n",
       "\n",
       "                                                                       cleaned  \\\n",
       "0  sai kha her kisi kay bus bat nhi lakin hal kal bi aj aj bi sirf aus say bus   \n",
       "1                                                                    sahi bt h   \n",
       "2                                                                           bt   \n",
       "3                                                                   wah je wah   \n",
       "4                                                             are wha kaya bat   \n",
       "\n",
       "   Sentiment_encoded  \n",
       "0                  2  \n",
       "1                  2  \n",
       "2                  2  \n",
       "3                  2  \n",
       "4                  2  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f24b732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df_pn[\"cleaned\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d50cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_pn[\"Sentiment_encoded\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7e71d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_encoded</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5287</td>\n",
       "      <td>5287</td>\n",
       "      <td>5287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6013</td>\n",
       "      <td>6013</td>\n",
       "      <td>6013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   text  Sentiment  cleaned\n",
       "Sentiment_encoded                          \n",
       "0                  5287       5287     5287\n",
       "2                  6013       6013     6013"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pn.groupby(\"Sentiment_encoded\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63378c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec8c5845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score 0.772566371681416\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=10, max_iter= 1000, solver=\"saga\", class_weight=\"balanced\", multi_class=\"multinomial\")  \n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Accuracy_score\" ,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b902537",
   "metadata": {},
   "source": [
    "Just weeding out the Neutral sentiment caused the accuracy to jump to 77% from 64%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a171a2",
   "metadata": {},
   "source": [
    "### Combine Positive and Neutral reviews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68544c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_posneu_neg = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9be822e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column based on a custom function\n",
    "df_posneu_neg['PN_Neg'] = df.apply(lambda row: 2 if row['Sentiment_encoded'] >= 1 else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a0d965e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>Sentiment_encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PN_Neg</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5287</td>\n",
       "      <td>5287</td>\n",
       "      <td>5287</td>\n",
       "      <td>5287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14941</td>\n",
       "      <td>14941</td>\n",
       "      <td>14941</td>\n",
       "      <td>14941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  Sentiment  cleaned  Sentiment_encoded\n",
       "PN_Neg                                              \n",
       "0        5287       5287     5287               5287\n",
       "2       14941      14941    14941              14941"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posneu_neg.groupby(\"PN_Neg\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "783e5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_posneu_neg[\"PN_Neg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd619f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df_posneu_neg[\"cleaned\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "738317e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20228, 32840)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63a089ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "684a06ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score 0.764211566979733\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=10, max_iter= 1000, solver=\"saga\", class_weight=\"balanced\", multi_class=\"multinomial\")  \n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Accuracy_score\" ,accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e00b5",
   "metadata": {},
   "source": [
    "By combining positive and neutral sentiment into one, we again observe a jump in accuracy to 76.4%. This is slightly lower \n",
    "than the accuracy obtained by using only positive and negative sentiment (77.25%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b6aad",
   "metadata": {},
   "source": [
    "It needs to be noted that each of these instances, we can do hyperparameter tuning for slightly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8e740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
